# ğŸ“ IntelliProject â€“ AI-Powered Academic Project Recommendation Engine

A clean, production-ready **FastAPI** backend that generates personalised
academic project ideas based on a student's skills, domain, difficulty, and
available time budget.

---

## ğŸ“ Project Structure

```
intelliproject/
â”‚
â”œâ”€â”€ main.py                     # FastAPI app instance, middleware, route registration
â”œâ”€â”€ run.py                      # Uvicorn entry point
â”œâ”€â”€ requirements.txt            # Backend dependencies
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes.py               # API endpoints (POST /api/v1/generate)
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py               # Centralized settings management
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py              # Pydantic request/response models
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ recommendation_service.py  # Business logic / AI engine
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ package.json
    â”œâ”€â”€ vite.config.js
    â””â”€â”€ src/
        â”œâ”€â”€ main.jsx
        â”œâ”€â”€ App.jsx
        â”œâ”€â”€ App.css
        â””â”€â”€ index.css
```

---

## ğŸš€ Step-by-Step Setup & Run

### 1. Clone / download the project

```bash
# If using git
git clone <your-repo-url>
cd intelliproject
```

### 2. Create a virtual environment

```bash
python -m venv venv

# Activate (macOS / Linux)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

```bash
cp .env.example .env
# Edit .env if you want to change the port or CORS origins
```

### 5. Start the server

```bash
python run.py
```

Or, using uvicorn directly:

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The server will be available at **http://localhost:8000**

---

## ğŸ“– Interactive API Docs

| Interface | URL |
|-----------|-----|
| Swagger UI | http://localhost:8000/docs |
| ReDoc | http://localhost:8000/redoc |
| Health check | http://localhost:8000/health |

---

## ğŸ”Œ API Usage

### `POST /api/v1/generate`

**Request body**

```json
{
  "skills": "Python, Machine Learning, REST APIs",
  "domain": "Healthcare",
  "difficulty": "Intermediate",
  "time_weeks": "8"
}
```

**Example with curl**

```bash
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "skills": "Python, Machine Learning, REST APIs",
    "domain": "Healthcare",
    "difficulty": "Intermediate",
    "time_weeks": "8"
  }'
```

**Response (trimmed)**

```json
{
  "status": "success",
  "input_summary": { ... },
  "recommendations": [
    {
      "title": "AI-Powered Healthcare Diagnostic Assistant",
      "problem_statement": "...",
      "tech_stack": ["Python 3.11", "FastAPI", "scikit-learn", "..."],
      "architecture": "...",
      "implementation_roadmap": ["Week 1â€“2: ...", "..."],
      "challenges": ["...", "..."],
      "resume_score": 98,
      "innovation_score": 92
    },
    { ... },
    { ... }
  ]
}
```

---

## ğŸ”§ Extending to a Real LLM

Open `app/services/recommendation_service.py` and replace the body of
`generate_projects()` with a call to your preferred provider:

```python
import openai  # or anthropic

def generate_projects(req: ProjectRequest) -> ProjectResponse:
    prompt = build_prompt(req)          # craft your prompt
    raw = openai.chat.completions.create(...)
    return parse_llm_response(raw)      # map to ProjectResponse
```

No changes needed in the route layer.

---

## âœ… Requirements

- Python 3.10+
- pip
